# Human Action Recognition in Video
<img src="images/exemplo.gif" width="400" height="400"/>

## Project Overview

This project consists of building a system capable of recognizing 20 different human actions.

**The 20 actions are:**

```
1. Amigo (LIBRAS Sign)
2. Carro (LIBRAS Sign)
3. Chuveiro (LIBRAS Sign)
4. Conhecer (LIBRAS Sign)
5. Coração
6. Curso (LIBRAS Sign)
7. Double Biceps
8. Esconder
9. Legal
10. LIBRAS (LIBRAS Sign)
11. Moto (LIBRAS Sign)
12. Obrigado (LIBRAS Sign)
13. Paz
14. Relaxado
15. Sem ação
16. Sentido
17. Tchau
18. Telefone (LIBRAS Sign)
19. Tomada (LIBRAS Sign)
20. Trabalhar (LIBRAS Sign)
```

## Summary

The MediaPipe framework was used to detect human poses. Once detected, the poses were passed as input to an MLP (Multi-Layer Perceptron) model for action classification.  
The training data for the MLP was generated by processing a video, extracting human poses frame by frame, and saving them in a CSV (Comma-Separated Values) file.

## Dataset

- The dataset consists of a `label` column representing actions, and the remaining columns contain the coordinates of the landmarks detected by MediaPipe, which serve as the classification model's features.
- It was created from videos stored in the `data` folder. *(Note: The `data` folder in this repository does not contain all the videos used in the project.)*

## Repository Contents

**The repository includes the following files:**

- `CreateDataset.py` → Generates the dataset from the input videos.
- `MLPModel.py` → Builds and trains the MLP model.
- `ActionDetection.py` → Reads a video (either recorded or live), detects landmarks, and uses the trained model to classify actions.
- `data` → Contains videos used to create the dataset of coordinates.
- `dataset` → Stores the coordinate dataset corresponding to the videos in the `data` folder.
- `outputs` and `images` → Auxiliary folders for storing project results.
- `requirements.txt` → Lists dependencies.

## Usage

### Initial Setup

1. Clone this repository to your computer using the following command:
   ```
   git clone https://github.com/MatheussAlvess/Human_Actions_Recognition.git
   ```
2. Navigate to the project directory.
3. Install the required dependencies by running:
  ```
   pip install -r requirements.txt
  ```

### Recognizing Actions in a Video
- **To recognize actions from an input video, run:**

  ```
  python ActionDetection.py <video_name>.mp4
  ```
    
  > Example: Running `python ActionDetection.py exemple.mp4` will save an output video named `output_exemple.mp4`.

### Real-Time Action Recognition
- **To recognize actions in real time using your webcam, run:**

  ```
  python ActionDetection.py live
  ```
  
  This will open the webcam and perform real-time action recognition.

  
___________________________________________
  
## Using This Project as a Base for Your Own:

**Once the repository is set up, follow these steps:**

1. Store your videos in a folder, naming each video according to the action it represents, as the class is derived from the filename.
   Example: `data/sign_language.mp4`   
2. Generate the coordinate dataset by running:
   ```
   python CreateDataset.py
   ```
   This will create a dataset from the videos found in the reference folder (default: `data`).
3. Train the MLP model using the generated dataset:
   ```
   python MLPModel.py
   ```
   (The model architecture and parameters can be modified within the script.)
4. Finally, run the action recognition command:
   ```
   python ActionDetection.py
   ``` 

___________________________________________

#### Notes and Limitations:

- This is the first version of the project, so action classifications may not be highly accurate in some cases due to several factors:
  1. **Limited Dataset:** Only one short video per action was considered, with little variation. For instance, if the model learns that the 'paz'
     action always occurs when one hand has two fingers raised while the other hand is not visible, it may incorrectly associate missing hands with the "paz" action.
  
  2. **Unprocessed Coordinate Dataset:**  In some cases, the "amigo" action did not detect either hand, leading the model to associate missing hands with the "amigo" action.
     
  3. **Model Optimization:** The MLP model was not fine-tuned. Some actions, like "Tchau," "paz," and "telefone," have very similar hand positions, making it harder for the model to distinguish them.
     
  4. **Landmark Detection Errors:** Based on the understanding gained during the execution of the project, the primary factor contributing to the confusion in action classification is the failure of landmark detection. Since the classifier model relies on these coordinates, any failure in landmark detection leads to confusion. This issue becomes even more critical during training, as the model may learn to associate the absence of hand coordinates with the 'telefone' gesture. To address this, the data can be processed more effectively, the video resolution can be improved to facilitate detection, or a more efficient detection model could be considered."



> [!TIP]
> - In a previous MediaPipe project, I encountered issues with landmark detection failures. As an alternative, I used YOLO Pose Estimation, which provides more reliable detections at a higher computational cost. [Cervical Posture - YOLO Pose Estimation](https://github.com/MatheussAlvess/Cervical_Posture_YOLO_Pose_Estimation).
> - The pose detection code is mainly based on this repository: [Body-Language-Decoder](https://github.com/nicknochnack/Body-Language-Decoder/tree/main).

> [!NOTE]
> - This project is part of a technical case evaluated for a Data Scientist position at HandTalk. Therefore, it is common to find comments in the code written in Portuguese.

<img src="images/paz_tchau.gif" width="400" height="400"/>
